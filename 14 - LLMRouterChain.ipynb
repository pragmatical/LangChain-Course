{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5284ede",
   "metadata": {},
   "source": [
    "# LLMRouterChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3fd50f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os \n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "openai.api_base= os.getenv('OPENAI_API_BASE')\n",
    "openai.api_type= \"azure\"\n",
    "openai.api_version = os.getenv('OPENAI_API_VERSION')\n",
    "deployment:str=os.getenv('CHATGPT_MODEL')\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=deployment,\n",
    "    model_name=deployment\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15220cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_template=\"\"\"You are a very smart human resources person. \\\n",
    "You are great at answering questions about human resources in a concise and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37f421da",
   "metadata": {},
   "outputs": [],
   "source": [
    "law_template=\"\"\"You are a very smart lawyer. \\\n",
    "You are great at answering questions about the law in a concise and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ca50521",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "{\n",
    "    'name':'human resources',\n",
    "    'description':'Answer human resources questions',\n",
    "    'template':hr_template},\n",
    "\n",
    "{\n",
    "    'name':'law',\n",
    "    'description':'Answers legal questions',\n",
    "    'template':law_template}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9131fefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_chains ={}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d4bec47",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p_info in prompt_infos:\n",
    "    name = p_info['name']\n",
    "    prompt_template = p_info['template']\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=llm,prompt=prompt)\n",
    "    destination_chains[name] = chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7b7fbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template('{input}')\n",
    "\n",
    "default_chain = LLMChain(llm=llm,prompt=default_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ae2dc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.\n",
      "\n",
      "<< FORMATTING >>\n",
      "Return a markdown code snippet with a JSON object formatted to look like:\n",
      "```json\n",
      "{{{{\n",
      "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
      "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
      "}}}}\n",
      "```\n",
      "\n",
      "REMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is not well suited for any of the candidate prompts.\n",
      "REMEMBER: \"next_inputs\" can just be the original input if you don't think any modifications are needed.\n",
      "\n",
      "<< CANDIDATE PROMPTS >>\n",
      "{destinations}\n",
      "\n",
      "<< INPUT >>\n",
      "{{input}}\n",
      "\n",
      "<< OUTPUT (must include ```json at the start of the response) >>\n",
      "<< OUTPUT (must end with ```) >>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(MULTI_PROMPT_ROUTER_TEMPLATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d266aab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "destinations=[\n",
    "f\"{p['name']}: {p['description']}\" for p in prompt_infos\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b12999c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['human resources: Answer human resources questions',\n",
       " 'law: Answers legal questions']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "destinations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfe483ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human resources: Answer human resources questions\n",
      "law: Answers legal questions\n"
     ]
    }
   ],
   "source": [
    "destination_str = \"\\n\".join(destinations)\n",
    "\n",
    "print(destination_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a597cda7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.\\n\\n<< FORMATTING >>\\nReturn a markdown code snippet with a JSON object formatted to look like:\\n```json\\n{{\\n    \"destination\": string \\\\ name of the prompt to use or \"DEFAULT\"\\n    \"next_inputs\": string \\\\ a potentially modified version of the original input\\n}}\\n```\\n\\nREMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is not well suited for any of the candidate prompts.\\nREMEMBER: \"next_inputs\" can just be the original input if you don\\'t think any modifications are needed.\\n\\n<< CANDIDATE PROMPTS >>\\nhuman resources: Answer human resources questions\\nlaw: Answers legal questions\\n\\n<< INPUT >>\\n{input}\\n\\n<< OUTPUT (must include ```json at the start of the response) >>\\n<< OUTPUT (must end with ```) >>\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destination_str)\n",
    "\n",
    "router_template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7c75f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jorgeluna/work/learning/langchain/LangChain-Course/venv-langchain-course/lib/python3.11/site-packages/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "law: {'input': 'How do intellectual property laws address the use of AI-generated content?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Intellectual property laws are still evolving in how they address the use of AI-generated content. Currently, the general rule is that the creator of the AI system that generates the content owns the copyright to that content. However, there may be situations where the user of the AI system could also claim some ownership rights if they made significant creative choices or inputs into the content creation process. Additionally, there are concerns about the potential infringement of existing copyrights if AI-generated content is too similar to existing works. Overall, it is important to seek legal guidance and stay up-to-date on developments in this area as AI technology continues to advance.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "router_prompt = PromptTemplate(template=router_template,\n",
    "input_variables=['input'],\n",
    "output_parser=RouterOutputParser())\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm,router_prompt)\n",
    "\n",
    "chain = MultiPromptChain(router_chain=router_chain,\n",
    "destination_chains=destination_chains,\n",
    "default_chain=default_chain,verbose=True)\n",
    "\n",
    "chain.run(\"How do intellectual property laws address the use of AI-generated content?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d26c254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jorgeluna/work/learning/langchain/LangChain-Course/venv-langchain-course/lib/python3.11/site-packages/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human resources: {'input': 'How do you handle conflict between team members?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'As an AI language model, I do not have personal experiences, but I can provide some general tips on handling conflicts between team members:\\n\\n1. Address the issue promptly: Ignoring the conflict will only make it worse. Address it as soon as possible before it escalates.\\n\\n2. Listen to both sides: Hear both parties out and understand their perspectives. This can help you identify the root cause of the conflict.\\n\\n3. Remain neutral: As an HR person, it is important to remain neutral and not take sides. Your role is to facilitate a resolution.\\n\\n4. Encourage open communication: Encourage both parties to openly communicate with each other and express their concerns.\\n\\n5. Find a mutually beneficial solution: Work with both parties to find a solution that works for everyone. This can involve compromises or finding common ground.\\n\\n6. Follow up: After the conflict is resolved, follow up with both parties to ensure that the solution is working and that there are no lingering issues.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"How do you handle conflict between team members?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d79918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b79ce20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
