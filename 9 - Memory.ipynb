{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "472a7aec",
   "metadata": {},
   "source": [
    "# Memory Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de456a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os \n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b2e55e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "openai.api_base= os.getenv('OPENAI_API_BASE')\n",
    "openai.api_type= \"azure\"\n",
    "openai.api_version = os.getenv('OPENAI_API_VERSION')\n",
    "deployment:str=os.getenv('CHATGPT_MODEL')\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=deployment,\n",
    "    model_name=deployment\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d93a76b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5f5aa79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Tom.  How are you today?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Tom! I'm doing great, thank you for asking. It's a beautiful day outside and I'm excited to chat with you. Is there something specific you'd like to talk about?\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input='Hi, my name is Tom.  How are you today?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3162a977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Tom.  How are you today?\n",
      "AI: Hello Tom! I'm doing great, thank you for asking. It's a beautiful day outside and I'm excited to chat with you. Is there something specific you'd like to talk about?\n",
      "Human: So what is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Your name is Tom, as you mentioned earlier. Is there anything else you'd like to know?\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input='So what is my name?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e30b5c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Human: Hi, my name is Tom.  How are you today?\\nAI: Hello Tom! I'm doing great, thank you for asking. It's a beautiful day outside and I'm excited to chat with you. Is there something specific you'd like to talk about?\\nHuman: So what is my name?\\nAI: Your name is Tom, as you mentioned earlier. Is there anything else you'd like to know?\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28b1244a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi, my name is Tom.  How are you today?\\nAI: Hello Tom! I'm doing great, thank you for asking. It's a beautiful day outside and I'm excited to chat with you. Is there something specific you'd like to talk about?\\nHuman: So what is my name?\\nAI: Your name is Tom, as you mentioned earlier. Is there anything else you'd like to know?\"}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76054619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Tom.  How are you today?\n",
      "AI: Hello Tom! I'm doing great, thank you for asking. It's a beautiful day outside and I'm excited to chat with you. Is there something specific you'd like to talk about?\n",
      "Human: So what is my name?\n",
      "AI: Your name is Tom, as you mentioned earlier. Is there anything else you'd like to know?\n",
      "Human: Tell me something funny.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Sure thing, Tom! Did you hear about the mathematician who's afraid of negative numbers? He'll stop at nothing to avoid them! Haha, I hope that made you laugh. Do you have any other questions for me?\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input='Tell me something funny.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daefe713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Human: Hi, my name is Tom.  How are you today?\\nAI: Hello Tom! I'm doing great, thank you for asking. It's a beautiful day outside and I'm excited to chat with you. Is there something specific you'd like to talk about?\\nHuman: So what is my name?\\nAI: Your name is Tom, as you mentioned earlier. Is there anything else you'd like to know?\\nHuman: Tell me something funny.\\nAI: Sure thing, Tom! Did you hear about the mathematician who's afraid of negative numbers? He'll stop at nothing to avoid them! Haha, I hope that made you laugh. Do you have any other questions for me?\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "080ab254",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({'input':'What type of typing system does Python employ: static or dynamic?'}, {'output': 'Dynamic'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bb64b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Human: Hi, my name is Tom.  How are you today?\\nAI: Hello Tom! I'm doing great, thank you for asking. It's a beautiful day outside and I'm excited to chat with you. Is there something specific you'd like to talk about?\\nHuman: So what is my name?\\nAI: Your name is Tom, as you mentioned earlier. Is there anything else you'd like to know?\\nHuman: Tell me something funny.\\nAI: Sure thing, Tom! Did you hear about the mathematician who's afraid of negative numbers? He'll stop at nothing to avoid them! Haha, I hope that made you laugh. Do you have any other questions for me?\\nHuman: What type of typing system does Python employ: static or dynamic?\\nAI: Dynamic\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70476fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "memory = ConversationBufferWindowMemory(k=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a55911ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7d54ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\":\"Hi, I recently purchased a product from your website, but I haven't received a confirmation email yet. Can you help?\"}, {\"output\": \"Hello, I'm sorry to hear that. Let me check the details for you. Can you provide your order number?\"})\n",
    "memory.save_context({\"input\": \"Sure, it's #12345.\"}, {\"output\": \"Thank you. I see your order. It seems there was a minor glitch. I've sent the confirmation email again. Please check.\"})\n",
    "memory.save_context({\"input\": \"Got it! Thanks for the quick resolution.\"}, {\"output\": \"You're welcome, Ivy! If you have any other concerns, feel free to ask. Have a great day!.\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29319b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbd0927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "015d6b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fbd706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39fbd7b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "get_num_tokens_from_messages() is not presently implemented for model gpt-35-turbo.See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/jorgeluna/work/learning/langchain/LangChain-Course/9 - Memory.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jorgeluna/work/learning/langchain/LangChain-Course/9%20-%20Memory.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m conversation\u001b[39m.\u001b[39;49mpredict(\u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mwhat is he best color.\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/work/learning/langchain/LangChain-Course/venv-langchain-course/lib/python3.11/site-packages/langchain/chains/llm.py:257\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, callbacks: Callbacks \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    243\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \n\u001b[1;32m    245\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs, callbacks\u001b[39m=\u001b[39;49mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m~/work/learning/langchain/LangChain-Course/venv-langchain-course/lib/python3.11/site-packages/langchain/chains/base.py:312\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    311\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m--> 312\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprep_outputs(\n\u001b[1;32m    313\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    314\u001b[0m )\n\u001b[1;32m    315\u001b[0m \u001b[39mif\u001b[39;00m include_run_info:\n\u001b[1;32m    316\u001b[0m     final_outputs[RUN_KEY] \u001b[39m=\u001b[39m RunInfo(run_id\u001b[39m=\u001b[39mrun_manager\u001b[39m.\u001b[39mrun_id)\n",
      "File \u001b[0;32m~/work/learning/langchain/LangChain-Course/venv-langchain-course/lib/python3.11/site-packages/langchain/chains/base.py:408\u001b[0m, in \u001b[0;36mChain.prep_outputs\u001b[0;34m(self, inputs, outputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_outputs(outputs)\n\u001b[1;32m    407\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 408\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmemory\u001b[39m.\u001b[39;49msave_context(inputs, outputs)\n\u001b[1;32m    409\u001b[0m \u001b[39mif\u001b[39;00m return_only_outputs:\n\u001b[1;32m    410\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/work/learning/langchain/LangChain-Course/venv-langchain-course/lib/python3.11/site-packages/langchain/memory/token_buffer.py:53\u001b[0m, in \u001b[0;36mConversationTokenBufferMemory.save_context\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39m# Prune buffer if it exceeds max token limit\u001b[39;00m\n\u001b[1;32m     52\u001b[0m buffer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_memory\u001b[39m.\u001b[39mmessages\n\u001b[0;32m---> 53\u001b[0m curr_buffer_length \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mget_num_tokens_from_messages(buffer)\n\u001b[1;32m     54\u001b[0m \u001b[39mif\u001b[39;00m curr_buffer_length \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_token_limit:\n\u001b[1;32m     55\u001b[0m     pruned_memory \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/work/learning/langchain/LangChain-Course/venv-langchain-course/lib/python3.11/site-packages/langchain/chat_models/openai.py:524\u001b[0m, in \u001b[0;36mChatOpenAI.get_num_tokens_from_messages\u001b[0;34m(self, messages)\u001b[0m\n\u001b[1;32m    522\u001b[0m     tokens_per_name \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    523\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 524\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    525\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mget_num_tokens_from_messages() is not presently implemented \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    526\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfor model \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    527\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSee https://github.com/openai/openai-python/blob/main/chatml.md for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minformation on how messages are converted to tokens.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    529\u001b[0m     )\n\u001b[1;32m    530\u001b[0m num_tokens \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    531\u001b[0m messages_dict \u001b[39m=\u001b[39m [convert_message_to_dict(m) \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m messages]\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: get_num_tokens_from_messages() is not presently implemented for model gpt-35-turbo.See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens."
     ]
    }
   ],
   "source": [
    "conversation.predict(input='what is he best color.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940de096",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "memory.buffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94b4f265",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "get_num_tokens_from_messages() is not presently implemented for model gpt-35-turbo.See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/jorgeluna/work/learning/langchain/LangChain-Course/9 - Memory.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jorgeluna/work/learning/langchain/LangChain-Course/9%20-%20Memory.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m conversation\u001b[39m.\u001b[39;49mpredict(\u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mWhat is the Python language?\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/work/learning/langchain/LangChain-Course/venv-langchain-course/lib/python3.11/site-packages/langchain/chains/llm.py:257\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, callbacks: Callbacks \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    243\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \n\u001b[1;32m    245\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs, callbacks\u001b[39m=\u001b[39;49mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m~/work/learning/langchain/LangChain-Course/venv-langchain-course/lib/python3.11/site-packages/langchain/chains/base.py:312\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    311\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m--> 312\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprep_outputs(\n\u001b[1;32m    313\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    314\u001b[0m )\n\u001b[1;32m    315\u001b[0m \u001b[39mif\u001b[39;00m include_run_info:\n\u001b[1;32m    316\u001b[0m     final_outputs[RUN_KEY] \u001b[39m=\u001b[39m RunInfo(run_id\u001b[39m=\u001b[39mrun_manager\u001b[39m.\u001b[39mrun_id)\n",
      "File \u001b[0;32m~/work/learning/langchain/LangChain-Course/venv-langchain-course/lib/python3.11/site-packages/langchain/chains/base.py:408\u001b[0m, in \u001b[0;36mChain.prep_outputs\u001b[0;34m(self, inputs, outputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_outputs(outputs)\n\u001b[1;32m    407\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 408\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmemory\u001b[39m.\u001b[39;49msave_context(inputs, outputs)\n\u001b[1;32m    409\u001b[0m \u001b[39mif\u001b[39;00m return_only_outputs:\n\u001b[1;32m    410\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/work/learning/langchain/LangChain-Course/venv-langchain-course/lib/python3.11/site-packages/langchain/memory/token_buffer.py:53\u001b[0m, in \u001b[0;36mConversationTokenBufferMemory.save_context\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39m# Prune buffer if it exceeds max token limit\u001b[39;00m\n\u001b[1;32m     52\u001b[0m buffer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_memory\u001b[39m.\u001b[39mmessages\n\u001b[0;32m---> 53\u001b[0m curr_buffer_length \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mget_num_tokens_from_messages(buffer)\n\u001b[1;32m     54\u001b[0m \u001b[39mif\u001b[39;00m curr_buffer_length \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_token_limit:\n\u001b[1;32m     55\u001b[0m     pruned_memory \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/work/learning/langchain/LangChain-Course/venv-langchain-course/lib/python3.11/site-packages/langchain/chat_models/openai.py:524\u001b[0m, in \u001b[0;36mChatOpenAI.get_num_tokens_from_messages\u001b[0;34m(self, messages)\u001b[0m\n\u001b[1;32m    522\u001b[0m     tokens_per_name \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    523\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 524\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    525\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mget_num_tokens_from_messages() is not presently implemented \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    526\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfor model \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    527\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSee https://github.com/openai/openai-python/blob/main/chatml.md for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minformation on how messages are converted to tokens.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    529\u001b[0m     )\n\u001b[1;32m    530\u001b[0m num_tokens \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    531\u001b[0m messages_dict \u001b[39m=\u001b[39m [convert_message_to_dict(m) \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m messages]\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: get_num_tokens_from_messages() is not presently implemented for model gpt-35-turbo.See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens."
     ]
    }
   ],
   "source": [
    "conversation.predict(input='What is the Python language?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b976885d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Human: what is he best color.\\nAI: I'm sorry, I can't answer that question as it's subjective and varies from person to person. However, some studies suggest that blue is the most popular color worldwide. Would you like me to provide more information on the psychology of colors?\\nHuman: What is the Python language?\\nAI: Python is a high-level, interpreted programming language that emphasizes code readability and simplicity. It was first released in 1991 and has since become one of the most popular programming languages in the world, known for its flexibility, ease of use, and versatility. It's widely used in web development, scientific computing, data analysis, artificial intelligence, machine learning, and many other fields. Python is open-source, which means it's free to use and modify, and has a large and active community of developers who contribute to its development and support.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025ffc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847967ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b9b7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb4f920",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hey, my name is Tom.\"}, {\"output\": \"Nice to meet you, Tom.  How are things with you?\"})\n",
    "memory.save_context({\"input\": \"I'm doing great! And you?\"}, {\"output\": \"The same!\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d1b2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.moving_summary_buffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e6a98e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
